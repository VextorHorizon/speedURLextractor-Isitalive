# ðŸŒ URL Suite: Extractor & Checker

A powerful set of Golang tools designed to extract URLs from webpages and verify their status (online/offline) with high performance and concurrency.

## ðŸ“ Project Structure

```text
url-extractor/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ url-extractor/   # Tool to extract URLs from HTML
â”‚   â”‚   â””â”€â”€ main.go
â”‚   â””â”€â”€ url-checker/     # Tool to check status of extracted URLs
â”‚       â””â”€â”€ main.go
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractor/       # Extraction logic (Regex based)
â”‚   â”œâ”€â”€ fetcher/         # HTTP content fetching
â”‚   â”œâ”€â”€ checker/         # URL status checking logic
â”‚   â””â”€â”€ models/          # Shared data structures
â”œâ”€â”€ urldata/             # Recommended directory for JSON results
â”œâ”€â”€ go.mod               # Go module definition
â””â”€â”€ README.md            # You are here!
```

---

## âš¡ Features

### ðŸ” URL Extractor
- âœ… Fetches content from any public URL.
- âœ… Extracts all links from `href` and `src` attributes.
- âœ… Resolves relative paths to absolute URLs.
- âœ… Automatic de-duplication of found URLs.
- âœ… Clean JSON output for automation.

### ðŸ§ª URL Checker
- âœ… High-concurrency status checking (Go routines).
- âœ… Customizable timeout and worker count.
- âœ… Color-coded CLI output for easy reading.
- âœ… Handles JSON files generated by the Extractor.
- âœ… Detailed summary report.

---

## ðŸ› ï¸ Installation

Build both tools into executable binaries:

```bash
# Build Extractor
go build -o url-extractor.exe ./cmd/url-extractor

# Build Checker
go build -o url-checker.exe ./cmd/url-checker
```

---

## ðŸš€ Usage Workflow

The tools are designed to work together: **Extract** first, then **Check**.

### 1. Extract URLs
Use the extractor to crawl a site and save the results to a file.

```bash
./url-extractor.exe https://example.com > urldata/example.json
```

### 2. Check URL Status
Use the checker to see which of those extracted URLs are actually working.

```bash
# Usage: url-checker <json-file> [concurrency] [timeout-seconds]
./url-checker.exe urldata/example.json 10 5
```

---

## ðŸ“– Detailed Commands

### URL Extractor
| Command | Description |
| :--- | :--- |
| `go run ./cmd/url-extractor/main.go <URL>` | Run without building |
| `./url-extractor.exe <URL>` | Run built binary |

### URL Checker
| Argument | Description | Default |
| :--- | :--- | :--- |
| `json-file` | The path to the JSON file (looks in `urldata/` automatically) | **Required** |
| `concurrency` | Number of simultaneous checks | `5` |
| `timeout` | Connection timeout in seconds | `10` |

**Example:**
```bash
./url-checker.exe target.json 20 2
```
*Checks `target.json` using 20 workers and a 2-second timeout.*

---

## ðŸ›¡ï¸ Safety & Ethical Scraping
This toolkit is designed with responsibility in mind:

- **Robots.txt Compliance**: Both tools automatically check for `robots.txt` before making requests. If a path is disallowed for crawlers, the tools will skip it to respect the site owner's wishes.
- **Smart Caching**: The `robots.txt` data is cached in memory per domain during a session, ensuring we don't spam the server with repeated policy requests.
- **Customizable Concurrency**: The Checker allows you to control the load on the target server. Please use a reasonable number of workers (e.g., 5-10) to avoid being mistaken for a DDoS attack.

---

## ðŸ“Š JSON Output Format
The extractor produces JSON in the following format:

```json
{
  "source_url": "https://example.com",
  "extracted_urls": [
    "https://example.com/about",
    "https://example.com/contact"
  ],
  "count": 2
}
```

---

## ðŸ“‹ Requirements
- **Go 1.21** or higher.
- Internet connection for fetching and checking live URLs.

---

## ðŸ’¡ Pro Tip
You can pipe errors to a log file to keep the JSON output clean:
```bash
./url-extractor.exe https://example.com > urldata/output.json 2>urldata/error.log
```
